/**
 * @file reinforcement-learning.ts
 * @description ê°•í™”í•™ìŠµ ê¸°ë°˜ AI - Q-Learning / DQN êµ¬í˜„
 *
 * ê¸°ëŠ¥:
 * - Q-Table ê¸°ë°˜ í•™ìŠµ (ì‘ì€ ìƒíƒœ ê³µê°„)
 * - ì‹ ê²½ë§ ê¸°ë°˜ DQN (í° ìƒíƒœ ê³µê°„)
 * - ê²½í—˜ ë¦¬í”Œë ˆì´
 * - Epsilon-Greedy íƒìƒ‰
 * - ëª¨ë¸ ì €ì¥/ë¡œë“œ
 */

import { writeFileSync, readFileSync, existsSync, mkdirSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';
import type { SimPlayerState, SimEnemyState, TokenState } from '../core/types';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// ==================== íƒ€ì… ì •ì˜ ====================

export interface GameState {
  playerHp: number;
  playerMaxHp: number;
  playerEther: number;
  playerBlock: number;
  playerTokens: TokenState;
  enemyHp: number;
  enemyMaxHp: number;
  enemyBlock: number;
  enemyTokens: TokenState;
  hand: string[];
  turn: number;
}

export interface Action {
  type: 'playCard' | 'endTurn';
  cardIndex?: number;
  cardId?: string;
}

export interface Experience {
  state: GameState;
  action: Action;
  reward: number;
  nextState: GameState | null;
  done: boolean;
}

export interface QLearningConfig {
  learningRate: number;      // í•™ìŠµë¥  (Î±)
  discountFactor: number;    // ê°ê°€ìœ¨ (Î³)
  epsilon: number;           // íƒìƒ‰ë¥ 
  epsilonDecay: number;      // íƒìƒ‰ë¥  ê°ì†Œ
  minEpsilon: number;        // ìµœì†Œ íƒìƒ‰ë¥ 
  batchSize: number;         // ë°°ì¹˜ í¬ê¸°
  replayBufferSize: number;  // ê²½í—˜ ë²„í¼ í¬ê¸°
}

// ==================== ìƒíƒœ ì¸ì½”ë”© ====================

export class StateEncoder {
  // ìƒíƒœë¥¼ ë¬¸ìì—´ í‚¤ë¡œ ë³€í™˜ (Q-Tableìš©)
  static encodeToKey(state: GameState): string {
    const hpBucket = Math.floor(state.playerHp / 10);
    const enemyHpBucket = Math.floor(state.enemyHp / 10);
    const etherBucket = state.playerEther;
    const blockBucket = Math.min(Math.floor(state.playerBlock / 5), 5);
    const turnBucket = Math.min(Math.floor(state.turn / 5), 4);

    return `${hpBucket}_${enemyHpBucket}_${etherBucket}_${blockBucket}_${turnBucket}`;
  }

  // ìƒíƒœë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë³€í™˜ (ì‹ ê²½ë§ìš©)
  static encodeToVector(state: GameState): number[] {
    const vector: number[] = [];

    // ì •ê·œí™”ëœ ìˆ˜ì¹˜ë“¤
    vector.push(state.playerHp / state.playerMaxHp);       // [0] í”Œë ˆì´ì–´ HP ë¹„ìœ¨
    vector.push(state.playerEther / 5);                     // [1] ì—í…Œë¥´ ë¹„ìœ¨
    vector.push(Math.min(state.playerBlock / 20, 1));      // [2] ë°©ì–´ë ¥ (ì •ê·œí™”)
    vector.push(state.enemyHp / state.enemyMaxHp);         // [3] ì  HP ë¹„ìœ¨
    vector.push(Math.min(state.enemyBlock / 20, 1));       // [4] ì  ë°©ì–´ë ¥

    // í† í° ìƒíƒœ
    vector.push(Math.min(state.playerTokens.offensive / 3, 1));
    vector.push(Math.min(state.playerTokens.defensive / 3, 1));
    vector.push(Math.min(state.playerTokens.vulnerable / 3, 1));
    vector.push(Math.min(state.playerTokens.weak / 3, 1));
    vector.push(Math.min(state.playerTokens.strength / 5, 1));

    vector.push(Math.min(state.enemyTokens.vulnerable / 3, 1));
    vector.push(Math.min(state.enemyTokens.weak / 3, 1));

    // í„´ ì •ë³´
    vector.push(Math.min(state.turn / 30, 1));

    // ì†íŒ¨ ì •ë³´ (ì›-í•« ì¸ì½”ë”© ëŒ€ì‹  ì¹´ë“œ íƒ€ì… ì¹´ìš´íŠ¸)
    const attackCount = state.hand.filter(c => c.includes('slash') || c.includes('Blow')).length;
    const defenseCount = state.hand.filter(c => c.includes('defend') || c.includes('Wall')).length;
    vector.push(attackCount / 5);
    vector.push(defenseCount / 5);
    vector.push(state.hand.length / 5);

    return vector;
  }

  // ì•¡ì…˜ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
  static encodeAction(action: Action, hand: string[]): number {
    if (action.type === 'endTurn') {
      return hand.length; // ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ëŠ” í„´ ì¢…ë£Œ
    }
    return action.cardIndex || 0;
  }

  // ì¸ë±ìŠ¤ë¥¼ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
  static decodeAction(index: number, hand: string[]): Action {
    if (index >= hand.length) {
      return { type: 'endTurn' };
    }
    return {
      type: 'playCard',
      cardIndex: index,
      cardId: hand[index],
    };
  }
}

// ==================== Q-Table ====================

export class QTable {
  private table: Map<string, number[]> = new Map();
  private actionCount: number;

  constructor(actionCount: number = 6) {
    this.actionCount = actionCount; // ìµœëŒ€ 5ì¥ + í„´ ì¢…ë£Œ
  }

  getQValues(stateKey: string): number[] {
    if (!this.table.has(stateKey)) {
      this.table.set(stateKey, new Array(this.actionCount).fill(0));
    }
    return this.table.get(stateKey)!;
  }

  setQValue(stateKey: string, actionIndex: number, value: number): void {
    const qValues = this.getQValues(stateKey);
    qValues[actionIndex] = value;
  }

  getBestAction(stateKey: string, validActions: number[]): number {
    const qValues = this.getQValues(stateKey);
    let bestAction = validActions[0];
    let bestValue = qValues[bestAction];

    for (const action of validActions) {
      if (qValues[action] > bestValue) {
        bestValue = qValues[action];
        bestAction = action;
      }
    }

    return bestAction;
  }

  size(): number {
    return this.table.size;
  }

  export(): Record<string, number[]> {
    return Object.fromEntries(this.table);
  }

  import(data: Record<string, number[]>): void {
    this.table = new Map(Object.entries(data));
  }
}

// ==================== ê°„ë‹¨í•œ ì‹ ê²½ë§ ====================

export class SimpleNeuralNetwork {
  private weights: number[][][];
  private biases: number[][];
  private layers: number[];

  constructor(layers: number[]) {
    this.layers = layers;
    this.weights = [];
    this.biases = [];

    // Xavier ì´ˆê¸°í™”
    for (let i = 0; i < layers.length - 1; i++) {
      const inputSize = layers[i];
      const outputSize = layers[i + 1];
      const scale = Math.sqrt(2 / (inputSize + outputSize));

      this.weights.push(
        Array(outputSize).fill(null).map(() =>
          Array(inputSize).fill(null).map(() => (Math.random() - 0.5) * 2 * scale)
        )
      );
      this.biases.push(Array(outputSize).fill(0));
    }
  }

  // ReLU í™œì„±í™”
  private relu(x: number): number {
    return Math.max(0, x);
  }

  // Softmax (ì¶œë ¥ì¸µ)
  private softmax(values: number[]): number[] {
    const max = Math.max(...values);
    const exp = values.map(v => Math.exp(v - max));
    const sum = exp.reduce((a, b) => a + b, 0);
    return exp.map(e => e / sum);
  }

  // ìˆœì „íŒŒ
  forward(input: number[]): number[] {
    let current = input;

    for (let i = 0; i < this.weights.length; i++) {
      const next: number[] = [];

      for (let j = 0; j < this.weights[i].length; j++) {
        let sum = this.biases[i][j];
        for (let k = 0; k < current.length; k++) {
          sum += this.weights[i][j][k] * current[k];
        }
        // ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ì•„ë‹ˆë©´ ReLU
        next.push(i < this.weights.length - 1 ? this.relu(sum) : sum);
      }

      current = next;
    }

    return current;
  }

  // ê°„ë‹¨í•œ SGD ì—…ë°ì´íŠ¸
  update(input: number[], targetIndex: number, targetValue: number, learningRate: number): void {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—­ì „íŒŒ ì‚¬ìš©
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ê·¼ì‚¬ ë°©ì‹ ì‚¬ìš©
    const output = this.forward(input);
    const error = targetValue - output[targetIndex];

    // ì¶œë ¥ì¸µ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    const lastLayer = this.weights.length - 1;
    for (let k = 0; k < this.weights[lastLayer][targetIndex].length; k++) {
      this.weights[lastLayer][targetIndex][k] += learningRate * error * input[k] * 0.01;
    }
    this.biases[lastLayer][targetIndex] += learningRate * error * 0.01;
  }

  // ê°€ì¤‘ì¹˜ ë³µì‚¬
  copyFrom(other: SimpleNeuralNetwork): void {
    for (let i = 0; i < this.weights.length; i++) {
      for (let j = 0; j < this.weights[i].length; j++) {
        for (let k = 0; k < this.weights[i][j].length; k++) {
          this.weights[i][j][k] = other.weights[i][j][k];
        }
        this.biases[i][j] = other.biases[i][j];
      }
    }
  }

  export(): { weights: number[][][]; biases: number[][]; layers: number[] } {
    return {
      weights: JSON.parse(JSON.stringify(this.weights)),
      biases: JSON.parse(JSON.stringify(this.biases)),
      layers: this.layers,
    };
  }

  import(data: { weights: number[][][]; biases: number[][] }): void {
    this.weights = data.weights;
    this.biases = data.biases;
  }
}

// ==================== ê²½í—˜ ë¦¬í”Œë ˆì´ ë²„í¼ ====================

export class ReplayBuffer {
  private buffer: Experience[] = [];
  private maxSize: number;

  constructor(maxSize: number = 10000) {
    this.maxSize = maxSize;
  }

  add(experience: Experience): void {
    this.buffer.push(experience);
    if (this.buffer.length > this.maxSize) {
      this.buffer.shift();
    }
  }

  sample(batchSize: number): Experience[] {
    const batch: Experience[] = [];
    const size = Math.min(batchSize, this.buffer.length);

    for (let i = 0; i < size; i++) {
      const index = Math.floor(Math.random() * this.buffer.length);
      batch.push(this.buffer[index]);
    }

    return batch;
  }

  size(): number {
    return this.buffer.length;
  }

  clear(): void {
    this.buffer = [];
  }
}

// ==================== Q-Learning ì—ì´ì „íŠ¸ ====================

export class QLearningAgent {
  private qTable: QTable;
  private config: QLearningConfig;
  private epsilon: number;
  private totalSteps: number = 0;

  constructor(config: Partial<QLearningConfig> = {}) {
    this.config = {
      learningRate: config.learningRate || 0.1,
      discountFactor: config.discountFactor || 0.95,
      epsilon: config.epsilon || 1.0,
      epsilonDecay: config.epsilonDecay || 0.995,
      minEpsilon: config.minEpsilon || 0.01,
      batchSize: config.batchSize || 32,
      replayBufferSize: config.replayBufferSize || 10000,
    };

    this.epsilon = this.config.epsilon;
    this.qTable = new QTable();
  }

  // í–‰ë™ ì„ íƒ (Epsilon-Greedy)
  selectAction(state: GameState, validActions: Action[]): Action {
    const stateKey = StateEncoder.encodeToKey(state);
    const validIndices = validActions.map((a, i) => i);

    // íƒìƒ‰ (Exploration)
    if (Math.random() < this.epsilon) {
      const randomIndex = Math.floor(Math.random() * validActions.length);
      return validActions[randomIndex];
    }

    // í™œìš© (Exploitation)
    const bestIndex = this.qTable.getBestAction(stateKey, validIndices);
    return validActions[bestIndex];
  }

  // í•™ìŠµ
  learn(experience: Experience): void {
    const stateKey = StateEncoder.encodeToKey(experience.state);
    const actionIndex = StateEncoder.encodeAction(experience.action, experience.state.hand);

    let target = experience.reward;

    if (!experience.done && experience.nextState) {
      const nextStateKey = StateEncoder.encodeToKey(experience.nextState);
      const nextQValues = this.qTable.getQValues(nextStateKey);
      const maxNextQ = Math.max(...nextQValues);
      target = experience.reward + this.config.discountFactor * maxNextQ;
    }

    const currentQ = this.qTable.getQValues(stateKey)[actionIndex];
    const newQ = currentQ + this.config.learningRate * (target - currentQ);
    this.qTable.setQValue(stateKey, actionIndex, newQ);

    this.totalSteps++;

    // Epsilon ê°ì†Œ
    this.epsilon = Math.max(
      this.config.minEpsilon,
      this.epsilon * this.config.epsilonDecay
    );
  }

  getStats(): { epsilon: number; qTableSize: number; totalSteps: number } {
    return {
      epsilon: this.epsilon,
      qTableSize: this.qTable.size(),
      totalSteps: this.totalSteps,
    };
  }

  save(filePath: string): void {
    const data = {
      qTable: this.qTable.export(),
      epsilon: this.epsilon,
      totalSteps: this.totalSteps,
      config: this.config,
    };
    writeFileSync(filePath, JSON.stringify(data, null, 2), 'utf-8');
  }

  load(filePath: string): void {
    if (!existsSync(filePath)) return;
    const data = JSON.parse(readFileSync(filePath, 'utf-8'));
    this.qTable.import(data.qTable);
    this.epsilon = data.epsilon;
    this.totalSteps = data.totalSteps;
    if (data.config) this.config = { ...this.config, ...data.config };
  }
}

// ==================== DQN ì—ì´ì „íŠ¸ ====================

export class DQNAgent {
  private network: SimpleNeuralNetwork;
  private targetNetwork: SimpleNeuralNetwork;
  private replayBuffer: ReplayBuffer;
  private config: QLearningConfig;
  private epsilon: number;
  private totalSteps: number = 0;
  private updateTargetEvery: number = 100;

  constructor(
    stateSize: number = 16,
    actionSize: number = 6,
    config: Partial<QLearningConfig> = {}
  ) {
    this.config = {
      learningRate: config.learningRate || 0.001,
      discountFactor: config.discountFactor || 0.99,
      epsilon: config.epsilon || 1.0,
      epsilonDecay: config.epsilonDecay || 0.999,
      minEpsilon: config.minEpsilon || 0.05,
      batchSize: config.batchSize || 64,
      replayBufferSize: config.replayBufferSize || 50000,
    };

    this.epsilon = this.config.epsilon;
    this.replayBuffer = new ReplayBuffer(this.config.replayBufferSize);

    // ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°: ì…ë ¥ â†’ 64 â†’ 64 â†’ ì¶œë ¥
    this.network = new SimpleNeuralNetwork([stateSize, 64, 64, actionSize]);
    this.targetNetwork = new SimpleNeuralNetwork([stateSize, 64, 64, actionSize]);
    this.targetNetwork.copyFrom(this.network);
  }

  // í–‰ë™ ì„ íƒ
  selectAction(state: GameState, validActions: Action[]): Action {
    // íƒìƒ‰
    if (Math.random() < this.epsilon) {
      const randomIndex = Math.floor(Math.random() * validActions.length);
      return validActions[randomIndex];
    }

    // í™œìš©
    const stateVector = StateEncoder.encodeToVector(state);
    const qValues = this.network.forward(stateVector);

    // ìœ íš¨í•œ ì•¡ì…˜ ì¤‘ ìµœëŒ€ Qê°’
    let bestIndex = 0;
    let bestValue = -Infinity;

    for (let i = 0; i < validActions.length; i++) {
      const actionIndex = StateEncoder.encodeAction(validActions[i], state.hand);
      if (qValues[actionIndex] > bestValue) {
        bestValue = qValues[actionIndex];
        bestIndex = i;
      }
    }

    return validActions[bestIndex];
  }

  // ê²½í—˜ ì €ì¥
  remember(experience: Experience): void {
    this.replayBuffer.add(experience);
  }

  // í•™ìŠµ
  learn(): void {
    if (this.replayBuffer.size() < this.config.batchSize) return;

    const batch = this.replayBuffer.sample(this.config.batchSize);

    for (const exp of batch) {
      const stateVector = StateEncoder.encodeToVector(exp.state);
      const actionIndex = StateEncoder.encodeAction(exp.action, exp.state.hand);

      let target = exp.reward;

      if (!exp.done && exp.nextState) {
        const nextStateVector = StateEncoder.encodeToVector(exp.nextState);
        const nextQValues = this.targetNetwork.forward(nextStateVector);
        target = exp.reward + this.config.discountFactor * Math.max(...nextQValues);
      }

      this.network.update(stateVector, actionIndex, target, this.config.learningRate);
    }

    this.totalSteps++;

    // Epsilon ê°ì†Œ
    this.epsilon = Math.max(
      this.config.minEpsilon,
      this.epsilon * this.config.epsilonDecay
    );

    // íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
    if (this.totalSteps % this.updateTargetEvery === 0) {
      this.targetNetwork.copyFrom(this.network);
    }
  }

  getStats(): { epsilon: number; bufferSize: number; totalSteps: number } {
    return {
      epsilon: this.epsilon,
      bufferSize: this.replayBuffer.size(),
      totalSteps: this.totalSteps,
    };
  }

  save(filePath: string): void {
    const data = {
      network: this.network.export(),
      targetNetwork: this.targetNetwork.export(),
      epsilon: this.epsilon,
      totalSteps: this.totalSteps,
      config: this.config,
    };
    writeFileSync(filePath, JSON.stringify(data), 'utf-8');
  }

  load(filePath: string): void {
    if (!existsSync(filePath)) return;
    const data = JSON.parse(readFileSync(filePath, 'utf-8'));
    this.network.import(data.network);
    this.targetNetwork.import(data.targetNetwork);
    this.epsilon = data.epsilon;
    this.totalSteps = data.totalSteps;
    if (data.config) this.config = { ...this.config, ...data.config };
  }
}

// ==================== ë³´ìƒ í•¨ìˆ˜ ====================

export class RewardCalculator {
  // ì „íˆ¬ ë³´ìƒ ê³„ì‚°
  static calculateBattleReward(
    prevState: GameState,
    currentState: GameState,
    actionTaken: Action,
    battleEnded: boolean,
    playerWon: boolean
  ): number {
    let reward = 0;

    // ìŠ¹/íŒ¨ ë³´ìƒ
    if (battleEnded) {
      reward += playerWon ? 100 : -50;
      return reward;
    }

    // HP ë³€í™” ë³´ìƒ
    const playerHpChange = currentState.playerHp - prevState.playerHp;
    const enemyHpChange = prevState.enemyHp - currentState.enemyHp;

    reward += enemyHpChange * 0.5;  // ì ì—ê²Œ í”¼í•´ë¥¼ ì¤Œ
    reward += playerHpChange * 0.3; // HP ì†ì‹¤ í˜ë„í‹°

    // ë°©ì–´ë ¥ ë³´ìƒ
    if (currentState.playerBlock > 0) {
      reward += 1;
    }

    // íš¨ìœ¨ì ì¸ ì¹´ë“œ ì‚¬ìš©
    if (actionTaken.type === 'playCard') {
      reward += 0.5;  // ì¹´ë“œ í”Œë ˆì´ ë³´ìƒ
    }

    // ìƒíƒœì´ìƒ ë¶€ì—¬ ë³´ìƒ
    if (currentState.enemyTokens.vulnerable > prevState.enemyTokens.vulnerable) {
      reward += 3;
    }
    if (currentState.enemyTokens.weak > prevState.enemyTokens.weak) {
      reward += 2;
    }

    // í”Œë ˆì´ì–´ ë²„í”„ ë³´ìƒ
    if (currentState.playerTokens.strength > prevState.playerTokens.strength) {
      reward += 2;
    }

    return reward;
  }

  // í„´ ì¢…ë£Œ ë³´ìƒ
  static calculateTurnEndReward(state: GameState): number {
    let reward = 0;

    // ë‚¨ì€ ì—í…Œë¥´ í˜ë„í‹° (íš¨ìœ¨ì  ì‚¬ìš© ì¥ë ¤)
    if (state.playerEther > 0) {
      reward -= state.playerEther * 0.5;
    }

    // ë¹ˆ ì† ë³´ë„ˆìŠ¤ (ëª¨ë“  ì¹´ë“œ ì‚¬ìš©)
    if (state.hand.length === 0) {
      reward += 2;
    }

    return reward;
  }
}

// ==================== í›ˆë ¨ ë§¤ë‹ˆì € ====================

export interface TrainingConfig {
  episodes: number;
  maxTurnsPerEpisode: number;
  saveEvery: number;
  logEvery: number;
}

export interface TrainingResult {
  episode: number;
  totalReward: number;
  turns: number;
  won: boolean;
  epsilon: number;
}

export class TrainingManager {
  private agent: QLearningAgent | DQNAgent;
  private config: TrainingConfig;
  private results: TrainingResult[] = [];
  private savePath: string;

  constructor(
    agent: QLearningAgent | DQNAgent,
    config: Partial<TrainingConfig> = {},
    savePath?: string
  ) {
    this.agent = agent;
    this.config = {
      episodes: config.episodes || 1000,
      maxTurnsPerEpisode: config.maxTurnsPerEpisode || 50,
      saveEvery: config.saveEvery || 100,
      logEvery: config.logEvery || 10,
    };
    this.savePath = savePath || join(__dirname, '../../data/models');

    if (!existsSync(this.savePath)) {
      mkdirSync(this.savePath, { recursive: true });
    }
  }

  // í›ˆë ¨ ì‹¤í–‰ (ì‹œë®¬ë ˆì´í„° ì½œë°± í•„ìš”)
  async train(
    createEpisode: () => { state: GameState; validActions: () => Action[] },
    step: (action: Action) => { nextState: GameState; reward: number; done: boolean; won: boolean },
    onEpisodeEnd?: (result: TrainingResult) => void
  ): Promise<TrainingResult[]> {
    console.log(`ğŸ® í›ˆë ¨ ì‹œì‘: ${this.config.episodes} ì—í”¼ì†Œë“œ`);

    for (let episode = 0; episode < this.config.episodes; episode++) {
      const { state: initialState, validActions } = createEpisode();
      let currentState = initialState;
      let totalReward = 0;
      let turns = 0;
      let done = false;
      let won = false;

      while (!done && turns < this.config.maxTurnsPerEpisode) {
        const actions = validActions();
        const action = this.agent.selectAction(currentState, actions);

        const { nextState, reward, done: isDone, won: isWon } = step(action);

        const experience: Experience = {
          state: currentState,
          action,
          reward,
          nextState: isDone ? null : nextState,
          done: isDone,
        };

        if (this.agent instanceof DQNAgent) {
          this.agent.remember(experience);
          this.agent.learn();
        } else {
          this.agent.learn(experience);
        }

        totalReward += reward;
        currentState = nextState;
        done = isDone;
        won = isWon;
        turns++;
      }

      const result: TrainingResult = {
        episode,
        totalReward,
        turns,
        won,
        epsilon: this.agent.getStats().epsilon,
      };

      this.results.push(result);
      onEpisodeEnd?.(result);

      // ë¡œê¹…
      if (episode % this.config.logEvery === 0) {
        const recentWinRate = this.calculateRecentWinRate(100);
        console.log(
          `ì—í”¼ì†Œë“œ ${episode}: ë³´ìƒ=${totalReward.toFixed(1)}, ` +
          `í„´=${turns}, ìŠ¹ë¦¬=${won}, Îµ=${result.epsilon.toFixed(3)}, ` +
          `ìµœê·¼ ìŠ¹ë¥ =${(recentWinRate * 100).toFixed(1)}%`
        );
      }

      // ì €ì¥
      if (episode % this.config.saveEvery === 0 && episode > 0) {
        this.saveCheckpoint(episode);
      }
    }

    console.log('âœ“ í›ˆë ¨ ì™„ë£Œ');
    return this.results;
  }

  private calculateRecentWinRate(count: number): number {
    const recent = this.results.slice(-count);
    if (recent.length === 0) return 0;
    return recent.filter(r => r.won).length / recent.length;
  }

  private saveCheckpoint(episode: number): void {
    const path = join(this.savePath, `checkpoint_${episode}.json`);
    this.agent.save(path);
    console.log(`ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: ${path}`);
  }

  getResults(): TrainingResult[] {
    return this.results;
  }

  generateReport(): string {
    const lines = [
      '# í›ˆë ¨ ë¦¬í¬íŠ¸',
      '',
      `ì´ ì—í”¼ì†Œë“œ: ${this.results.length}`,
      `ìµœì¢… ìŠ¹ë¥ : ${(this.calculateRecentWinRate(100) * 100).toFixed(1)}%`,
      '',
      '## ì§„í–‰ ì¶”ì´',
      '',
      '| ì—í”¼ì†Œë“œ | ë³´ìƒ | í„´ | ìŠ¹ë¦¬ | Epsilon |',
      '|----------|------|-----|------|---------|',
    ];

    // ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ 10ê°œ êµ¬ê°„ìœ¼ë¡œ)
    const step = Math.max(1, Math.floor(this.results.length / 10));
    for (let i = 0; i < this.results.length; i += step) {
      const r = this.results[i];
      lines.push(
        `| ${r.episode} | ${r.totalReward.toFixed(1)} | ${r.turns} | ` +
        `${r.won ? 'âœ“' : 'âœ—'} | ${r.epsilon.toFixed(3)} |`
      );
    }

    return lines.join('\n');
  }
}

// ==================== ê¸°ë³¸ ë‚´ë³´ë‚´ê¸° ====================

export function createDefaultAgent(type: 'qlearning' | 'dqn' = 'qlearning'): QLearningAgent | DQNAgent {
  if (type === 'dqn') {
    return new DQNAgent();
  }
  return new QLearningAgent();
}

export function getDefaultModelPath(): string {
  return join(__dirname, '../../data/models');
}
